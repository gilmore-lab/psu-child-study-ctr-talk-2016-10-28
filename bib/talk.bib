
@article{collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/349/6251/aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. Related ResourcesIn Science Magazine Podcasts Science Podcast: 28 August Show Science 28 August 2015: 999.In Depth Reproducibility Many psychology papers fail replication test John BohannonScience 28 August 2015: 910-911.Policy Forum Scientific Standards Promoting an open research culture B. A. Nosek et al.Science 26 June 2015: 1422-1425.Editorial EDITORIAL: Solving reproducibility Stuart BuckScience 26 June 2015: 1403.More related articles... View larger version: In this page In a new window Download PowerPoint Slide for Teaching Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716},
	language = {en},
	number = {6251},
	urldate = {2015-08-28},
	journal = {Science},
	author = {Collaboration, Open Science},
	month = aug,
	year = {2015},
	pages = {aac4716},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/M2DUJAI7/Collaboration - 2015 - Estimating the reproducibility of psychological.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/IN75VC96/aac4716.html:text/html}
}


@article{maxwell_persistence_2004,
	title = {The {Persistence} of {Underpowered} {Studies} in {Psychological} {Research}: {Causes}, {Consequences}, and {Remedies}},
	volume = {9},
	copyright = {(c) 2012 APA, all rights reserved},
	issn = {1939-1463(Electronic);1082-989X(Print)},
	shorttitle = {The {Persistence} of {Underpowered} {Studies} in {Psychological} {Research}},
	doi = {10.1037/1082-989X.9.2.147},
	abstract = {Underpowered studies persist in the psychological literature. This article examines reasons for their persistence and the effects on efforts to create a cumulative science. The "curse of multiplicities" plays a central role in the presentation. Most psychologists realize that testing multiple hypotheses in a single study affects the Type I error rate, but corresponding implications for power have largely been ignored. The presence of multiple hypothesis tests leads to 3 different conceptualizations of power. Implications of these 3 conceptualizations are discussed from the perspective of the individual researcher and from the perspective of developing a coherent literature. Supplementing significance tests with effect size measures and confidence intervals is shown to address some but not necessarily all problems associated with multiple testing.},
	number = {2},
	journal = {Psychological Methods},
	author = {Maxwell, Scott E.},
	year = {2004},
	keywords = {*Effect Size (Statistical), *Hypothesis Testing, *Methodology, *Psychology, *Statistical Power, Confidence Limits (Statistics), Type I Errors},
	pages = {147--163}
}


@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	url = {http://dx.doi.org/10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Published research findings are sometimes refuted by subsequent evidence, says Ioannidis, with ensuing confusion and disappointment.},
	number = {8},
	urldate = {2015-07-27},
	journal = {PLoS Med},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	pages = {e124},
	file = {PLoS Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/HIEJMJ4S/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf}
}


@article{szucs_empirical_2016,
	title = {Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature},
	copyright = {© 2016, Published by Cold Spring Harbor Laboratory Press. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {http://biorxiv.org/content/early/2016/08/25/071530},
	doi = {10.1101/071530},
	abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by extracting more than 100,000 statistical records from about 10,000 cognitive neuroscience and psychology papers published during the past 5 years. The reported median effect size was d=0.93 (inter-quartile range: 0.64-1.46) for nominally statistically significant results and d=0.24 (0.11-0.42) for non-significant results. Median power to detect small, medium and large effects was 0.12, 0.44 and 0.73, reflecting no improvement through the past half-century. Power was lowest for cognitive neuroscience journals. 14\% of papers reported some statistically significant results, although the respective F statistic and degrees of freedom proved that these were non-significant; p value errors positively correlated with journal impact factors. False report probability is likely to exceed 50\% for the whole literature. In light of our findings the recently reported low replication success in psychology is realistic and worse performance may be expected for cognitive neuroscience.},
	language = {en},
	urldate = {2016-10-02},
	journal = {bioRxiv},
	author = {Szucs, Denes and Ioannidis, John PA},
	month = aug,
	year = {2016},
	pages = {071530},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/VS2PGZCM/Szucs and Ioannidis - 2016 - Empirical assessment of published effect sizes and.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/HUEFQC2G/071530.html:text/html}
}

@article{henrich_weirdest_2010,
	title = {The weirdest people in the world?},
	volume = {33},
	issn = {1469-1825},
	doi = {10.1017/S0140525X0999152X},
	abstract = {Behavioral scientists routinely publish broad claims about human psychology and behavior in the world's top journals based on samples drawn entirely from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies. Researchers - often implicitly - assume that either there is little variation across human populations, or that these "standard subjects" are as representative of the species as any other population. Are these assumptions justified? Here, our review of the comparative database from across the behavioral sciences suggests both that there is substantial variability in experimental results across populations and that WEIRD subjects are particularly unusual compared with the rest of the species - frequent outliers. The domains reviewed include visual perception, fairness, cooperation, spatial reasoning, categorization and inferential induction, moral reasoning, reasoning styles, self-concepts and related motivations, and the heritability of IQ. The findings suggest that members of WEIRD societies, including young children, are among the least representative populations one could find for generalizing about humans. Many of these findings involve domains that are associated with fundamental aspects of psychology, motivation, and behavior - hence, there are no obvious a priori grounds for claiming that a particular behavioral phenomenon is universal based on sampling from a single subpopulation. Overall, these empirical patterns suggests that we need to be less cavalier in addressing questions of human nature on the basis of data drawn from this particularly thin, and rather unusual, slice of humanity. We close by proposing ways to structurally re-organize the behavioral sciences to best tackle these challenges.},
	language = {eng},
	number = {2-3},
	journal = {The Behavioral and Brain Sciences},
	author = {Henrich, Joseph and Heine, Steven J. and Norenzayan, Ara},
	month = jun,
	year = {2010},
	pmid = {20550733},
	keywords = {Behavioral Sciences, Cognition, Cross-Cultural Comparison, decision making, Humans, Morals, Population Groups, Visual Perception},
	pages = {61--83; discussion 83--135}
}

@article{goodman_what_2016,
	title = {What does research reproducibility mean?},
	volume = {8},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {1946-6234, 1946-6242},
	url = {http://stm.sciencemag.org/content/8/341/341ps12},
	doi = {10.1126/scitranslmed.aaf5027},
	abstract = {The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.},
	language = {en},
	number = {341},
	urldate = {2016-10-09},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	pmid = {27252173},
	pages = {341ps12--341ps12},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/M2JX5V9V/Goodman et al. - 2016 - What does research reproducibility mean.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/TNSTI5G9/341ps12.html:text/html}
}